{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":252411904},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1596712741620,"exec_count":1,"id":"6c75e2","input":"import itertools\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.nn.init as init\nfrom torch import nn\nfrom torch import distributions\nfrom torch.distributions import MultivariateNormal, Uniform, TransformedDistribution, SigmoidTransform\nfrom torch.nn.parameter import Parameter\n\nimport sys; sys.path.append('../../../share/data/')\n\nfrom nflib.flows import (\n    AffineConstantFlow, ActNorm, AffineHalfFlow, \n    SlowMAF, MAF, IAF, Invertible1x1Conv,\n    NormalizingFlow, NormalizingFlowModel,\n)\nfrom nflib.spline_flows import NSF_AR, NSF_CL\n\n# for auto-reloading external modules\n# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n%load_ext autoreload\n%autoreload 2","kernel":"python3","pos":1,"start":1596712740467,"state":"done","type":"cell"}
{"cell_type":"code","end":1596712745486,"exec_count":2,"id":"e40c10","input":"# Lightweight datasets\nimport pickle\nfrom sklearn import datasets\n\nclass DatasetSIGGRAPH:\n    \"\"\"\n    haha, found from Eric https://blog.evjang.com/2018/01/nf2.html\n    https://github.com/ericjang/normalizing-flows-tutorial/blob/master/siggraph.pkl\n    \"\"\"\n    def __init__(self):\n        with open('siggraph.pkl', 'rb') as f:\n            XY = np.array(pickle.load(f), dtype=np.float32)\n            XY -= np.mean(XY, axis=0) # center\n        self.XY = torch.from_numpy(XY)\n\n    def sample(self, n):\n        X = self.XY[np.random.randint(self.XY.shape[0], size=n)]\n        return X\n\nclass DatasetMoons:\n    \"\"\" two half-moons \"\"\"\n    def sample(self, n):\n        moons = datasets.make_moons(n_samples=n, noise=0.05)[0].astype(np.float32)\n        return torch.from_numpy(moons)\n\nclass DatasetMixture:\n    \"\"\" 4 mixture of gaussians \"\"\"\n    def sample(self, n):\n        assert n%4 == 0\n        r = np.r_[np.random.randn(n // 4, 2)*0.5 + np.array([0, -2]),\n                  np.random.randn(n // 4, 2)*0.5 + np.array([0, 0]),\n                  np.random.randn(n // 4, 2)*0.5 + np.array([2, 2]),\n                  np.random.randn(n // 4, 2)*0.5 + np.array([-2, 2])]\n        return torch.from_numpy(r.astype(np.float32))\n\nd = DatasetMoons()\n#d = DatasetMixture()\n#d = DatasetSIGGRAPH()\n\n# sample x using d\n# YOUR CODE HERE\nx = d.sample(128)\n\nplt.figure(figsize=(4,4))\nplt.scatter(x[:,0], x[:,1], s=5, alpha=0.5)\nplt.axis('equal')","kernel":"python3","metadata":{"nbgrader":{"cell_type":"code","checksum":"e2826af6cd00e1bc41c0ac4c32bd78f6","grade":false,"grade_id":"e40c10","locked":false,"schema_version":3,"solution":true,"task":false}},"output":{"0":{"data":{"text/plain":"(-1.183898490667343,\n 2.201866310834885,\n -0.6151307612657547,\n 1.1458571344614028)"},"exec_count":2},"1":{"data":{"image/png":"7f341b6b2c5cfce80bc23fdeef2f4353b46f1b57","text/plain":"<Figure size 288x288 with 1 Axes>"}}},"pos":8,"start":1596712744847,"state":"done","type":"cell"}
{"cell_type":"code","end":1596712745758,"exec_count":3,"id":"8f5d00","input":"from nose.tools import assert_equal\nassert_equal(x.shape, (128,2))","kernel":"python3","metadata":{"deletable":false,"editable":false,"nbgrader":{"grade":true,"grade_id":"8f5d00","locked":true,"points":1,"schema_version":3,"solution":false,"task":false}},"pos":9,"start":1596712745725,"state":"done","type":"cell"}
{"cell_type":"code","end":1596712746145,"exec_count":4,"id":"233f6c","input":"# construct a model\n# prior = MultivariateNormal(torch.zeros(2), torch.eye(2))\nprior = TransformedDistribution(Uniform(torch.zeros(2), torch.ones(2)), SigmoidTransform().inv) # Logistic distribution\n\n# RealNVP\nflows = [AffineHalfFlow(dim=2, parity=i%2) for i in range(9)]\n\n# NICE\n# flows = [AffineHalfFlow(dim=2, parity=i%2, scale=False) for i in range(4)]\n# flows.append(AffineConstantFlow(dim=2, shift=False))\n\n# SlowMAF (MAF, but without any parameter sharing for each dimension's scale/shift)\n# flows = [SlowMAF(dim=2, parity=i%2) for i in range(4)]\n\n# MAF (with MADE net, so we get very fast density estimation)\n# flows = [MAF(dim=2, parity=i%2) for i in range(4)]\n\n# IAF (with MADE net, so we get very fast sampling)\n# flows = [IAF(dim=2, parity=i%2) for i in range(3)]\n\n# insert ActNorms to any of the flows above\n# norms = [ActNorm(dim=2) for _ in flows]\n# flows = list(itertools.chain(*zip(norms, flows)))\n\n# Glow paper\n# flows = [Invertible1x1Conv(dim=2) for i in range(3)]\n# norms = [ActNorm(dim=2) for _ in flows]\n# couplings = [AffineHalfFlow(dim=2, parity=i%2, nh=32) for i in range(len(flows))]\n# flows = list(itertools.chain(*zip(norms, flows, couplings))) # append a coupling layer after each 1x1\n\n# Neural splines, coupling\n# nfs_flow = NSF_CL if True else NSF_AR\n# flows = [nfs_flow(dim=2, K=8, B=3, hidden_dim=16) for _ in range(3)]\n# convs = [Invertible1x1Conv(dim=2) for _ in flows]\nnorms = [ActNorm(dim=2) for _ in flows]\n# flows = list(itertools.chain(*zip(norms, convs, flows)))\nflows = list(itertools.chain(*zip(norms, flows)))\n\n# construct the model\nmodel = NormalizingFlowModel(prior, flows)","kernel":"python3","pos":10,"start":1596712746112,"state":"done","type":"cell"}
{"cell_type":"code","end":1596712747887,"exec_count":5,"id":"3aa353","input":"# optimizer\noptimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5) # todo tune WD\nprint(\"number of params: \", sum(p.numel() for p in model.parameters()))","kernel":"python3","output":{"0":{"name":"stdout","text":"number of params:  22950\n"}},"pos":11,"start":1596712747868,"state":"done","type":"cell"}
{"cell_type":"code","end":1596712749582,"exec_count":6,"id":"1f0c25","input":"model.train()\nfor k in range(1000):\n    # sample x\n    # YOUR CODE HERE\n    raise NotImplementedError()\n\n    zs, prior_logprob, log_det = model(x)\n    logprob = prior_logprob + log_det\n    # define loss = NLL (negative log likelihood)\n    # YOUR CODE HERE\n    loss = \n\n    # gradient step\n    # YOUR CODE HERE\n    raise NotImplementedError()\n\n    if k % 100 == 0:\n        print(loss.item())\n","kernel":"python3","metadata":{"nbgrader":{"cell_type":"code","checksum":"190793de620a2f2dce4f6953915bf503","grade":false,"grade_id":"1f0c25","locked":false,"schema_version":3,"solution":true,"task":false}},"output":{"0":{"ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-6-df635f48fe9b>, line 11)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-df635f48fe9b>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    loss =\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}},"pos":12,"start":1596712749564,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"5bc7df","input":"","pos":17,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"7cea08","input":"assert loss.item() > 0","metadata":{"deletable":false,"editable":false,"nbgrader":{"grade":true,"grade_id":"7cea08","locked":true,"points":1,"schema_version":3,"solution":false,"task":false}},"pos":13,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"ad46d7","input":"model.eval()\n\nplt.figure(figsize=(17,5))\n\nx = d.sample(128)\nzs, prior_logprob, log_det = model(x)\nz = zs[-1]\n\nx = x.detach().numpy()\nz = z.detach().numpy()\np = model.prior.sample([128, 2]).squeeze()\nplt.subplot(131)\nplt.scatter(p[:,0], p[:,1], c='g', s=5)\nplt.scatter(z[:,0], z[:,1], c='r', s=5)\nplt.legend(['prior', 'x->z', 'data'])\nplt.axis('scaled')\nplt.title('x -> z')\n\nzs = model.sample(128*8)\nz = zs[-1]\nz = z.detach().numpy()\nplt.subplot(132)\nplt.scatter(x[:,0], x[:,1], c='b', s=5, alpha=0.5)\nplt.scatter(z[:,0], z[:,1], c='r', s=5, alpha=0.5)\nplt.legend(['data', 'z->x'])\nplt.axis('scaled')\nplt.title('z -> x')\n\nplt.subplot(133)\nng = 100\nxx, yy = np.linspace(-3, 3, ng), np.linspace(-3, 3, ng)\nxv, yv = np.meshgrid(xx, yy)\nxy = np.stack([xv, yv], axis=-1)\nxy = xy.reshape((ng*ng, 2))\nxy = torch.from_numpy(xy).float()\nzs, prior_logprob, log_det = model(xy)\nplt.scatter(xy[:,0], xy[:,1], c=np.nan_to_num(prior_logprob.detach().exp()))\nplt.title('density')","pos":14,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"e974f5","input":"\n# train and render\n# code duplication because it's very late at night now and i'm tired\n\nimport matplotlib.gridspec as gridspec\n\nng = 20\nxx, yy = np.linspace(-3, 3, ng), np.linspace(-3, 3, ng)\nxv, yv = np.meshgrid(xx, yy)\nxy = np.stack([xv, yv], axis=-1)\nin_circle = np.sqrt((xy**2).sum(axis=2)) <= 3\nxy = xy.reshape((ng*ng, 2))\nxy = torch.from_numpy(xy.astype(np.float32))\n\nxval = d.sample(128*5)\n\nmodel.train()\nfor k in range(500):\n    \n    # sample\n    x = d.sample(128)\n    \n    # train a bit\n    zs, prior_logprob, log_det = model(x)\n    logprob = prior_logprob + log_det\n    loss = -torch.sum(logprob) # NLL\n    model.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if k % 10 == 0:\n        # vis\n        zs, log_det = model.backward(xy)\n        backward_flow_names = [type(f).__name__ for f in model.flow.flows[::-1]]\n        nz = len(zs)\n        i = nz - 1 - 1\n\n        z0 = zs[i].detach().numpy()\n        z1 = zs[i+1].detach().numpy()\n\n        # plot how the samples travel at this stage\n        ss = 0.1\n        fig = plt.figure(figsize=(10, 5))\n        outer = gridspec.GridSpec(1, 2, wspace=ss, hspace=ss)\n        inner1 = gridspec.GridSpecFromSubplotSpec(3, 3, subplot_spec=outer[0], wspace=ss, hspace=ss)\n        inner2 = gridspec.GridSpecFromSubplotSpec(1, 1, subplot_spec=outer[1], wspace=ss, hspace=ss)\n        \n        backward_flow_names = [type(f).__name__ for f in model.flow.flows[::-1]]\n        nz = len(zs)\n        for i in range(min(nz-1, 9)):\n            ax = plt.Subplot(fig, inner1[i])\n            z0 = zs[i].detach().numpy()\n            z1 = zs[i+1].detach().numpy()\n            ax.scatter(z0[:,0], z0[:, 1], c='r', s=1, alpha=0.5)\n            ax.scatter(z1[:,0], z1[:, 1], c='b', s=1, alpha=0.5)\n            ax.quiver(z0[:,0], z0[:,1], z1[:,0] - z0[:,0], z1[:,1] - z0[:,1], units='xy', scale=1, alpha=0.5)\n            ax.axis([-3, 3, -3, 3])\n            ax.set_yticklabels([])\n            ax.set_xticklabels([])\n            #ax.set_title(\"layer %d -> %d (%s)\" % (i, i+1, backward_flow_names[i]))\n            fig.add_subplot(ax)\n        \n        ax = plt.Subplot(fig, inner2[0])\n        q = z1.reshape((ng, ng, 2))\n        # y coords\n        p1 = np.reshape(q[1:,:,:], (ng**2-ng,2))\n        p2 = np.reshape(q[:-1,:,:], (ng**2-ng,2))\n        inc = np.reshape(in_circle[1:,:] | in_circle[:-1,:], (ng**2-ng,))\n        p1, p2 = p1[inc], p2[inc]\n        lcy = mc.LineCollection(zip(p1, p2), linewidths=1, alpha=0.5, color='k')\n        # x coords\n        p1 = np.reshape(q[:,1:,:], (ng**2-ng,2))\n        p2 = np.reshape(q[:,:-1,:], (ng**2-ng,2))\n        inc = np.reshape(in_circle[:,1:] | in_circle[:,:-1], (ng**2-ng,))\n        p1, p2 = p1[inc], p2[inc]\n        lcx = mc.LineCollection(zip(p1, p2), linewidths=1, alpha=0.5, color='k')\n        # draw the lines\n        ax.add_collection(lcy)\n        ax.add_collection(lcx)\n        ax.axis([-3, 3, -3, 3])\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])\n        #ax.set_title(\"grid warp at the end of %d\" % (i+1,))\n        fig.add_subplot(ax)\n        \n        # draw the data too\n        plt.scatter(xval[:,0], xval[:,1], c='r', s=5, alpha=0.5)\n        \n        break\n        #fname = 'out/step_%04d.png' % (k,)\n        #plt.savefig(fname, dpi=200)\n        #print(\"saved\", fname, 'loss', loss.item())\n        #plt.close(fig)\n        ","pos":16,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"fb7aa1","input":"# Visualize the step-wise flow in the full net\nfrom matplotlib import collections  as mc\n\n# plot the coordinate warp\nng = 20\nxx, yy = np.linspace(-3, 3, ng), np.linspace(-3, 3, ng)\nxv, yv = np.meshgrid(xx, yy)\nxy = np.stack([xv, yv], axis=-1)\nin_circle = np.sqrt((xy**2).sum(axis=2)) <= 3 # seems appropriate since we use radial distributions as priors\nxy = xy.reshape((ng*ng, 2))\nxy = torch.from_numpy(xy.astype(np.float32))\n\nzs, log_det = model.backward(xy)\n\nbackward_flow_names = [type(f).__name__ for f in model.flow.flows[::-1]]\nnz = len(zs)\nfor i in range(nz - 1):\n    z0 = zs[i].detach().numpy()\n    z1 = zs[i+1].detach().numpy()\n    \n    # plot how the samples travel at this stage\n    figs, axs = plt.subplots(1, 2, figsize=(6, 3))\n    #plt.figure(figsize=(20,10))\n    axs[0].scatter(z0[:,0], z0[:, 1], c='r', s=3)\n    axs[0].scatter(z1[:,0], z1[:, 1], c='b', s=3)\n    axs[0].quiver(z0[:,0], z0[:,1], z1[:,0] - z0[:,0], z1[:,1] - z0[:,1], units='xy', scale=1, alpha=0.5)\n    axs[0].axis([-3, 3, -3, 3])\n    axs[0].set_title(\"layer %d -> %d (%s)\" % (i, i+1, backward_flow_names[i]))\n    \n    q = z1.reshape((ng, ng, 2))\n    # y coords\n    p1 = np.reshape(q[1:,:,:], (ng**2-ng,2))\n    p2 = np.reshape(q[:-1,:,:], (ng**2-ng,2))\n    inc = np.reshape(in_circle[1:,:] | in_circle[:-1,:], (ng**2-ng,))\n    p1, p2 = p1[inc], p2[inc]\n    lcy = mc.LineCollection(zip(p1, p2), linewidths=1, alpha=0.5, color='k')\n    # x coords\n    p1 = np.reshape(q[:,1:,:], (ng**2-ng,2))\n    p2 = np.reshape(q[:,:-1,:], (ng**2-ng,2))\n    inc = np.reshape(in_circle[:,1:] | in_circle[:,:-1], (ng**2-ng,))\n    p1, p2 = p1[inc], p2[inc]\n    lcx = mc.LineCollection(zip(p1, p2), linewidths=1, alpha=0.5, color='k')\n    # draw the lines\n    axs[1].add_collection(lcy)\n    axs[1].add_collection(lcx)\n    axs[1].axis([-3, 3, -3, 3])\n    axs[1].set_title(\"grid warp at the end of %d\" % (i+1,))\n    \n    # draw the data too\n    plt.scatter(x[:,0], x[:,1], c='r', s=5, alpha=0.5)\n    \n","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"12bee0","input":"Code is taken from https://github.com/karpathy/pytorch-normalizing-flows","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"480066","input":"# Intro\n**Problem:** how to estimate the pdf of final distribution at each point?\n\n\n**Idea:** Let's define the bijection $z_k=f(z_0)$ between simple distribution of $z_0$ with known pdf and our distribution $z_k$ with unknown pdf\n\n\n![](https://2.bp.blogspot.com/-g37e2x1miRo/Wl-g8ajU11I/AAAAAAAAHkY/PbIorxOav_Y61yFJeXsQLRlcKTzlkykYwCLcBGAs/s1600/shakir_danilo_slide.png)","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"724b2b","input":"# Example\n![](http://akosiorek.github.io/resources/simple_flows.png)","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"744e36","input":"# Example of transformations\n- Planar flows; $f(x) = x + u h(w^\\intercal z + b)$\n- Radial flows; $f(x) = x + \\frac{\\beta}{\\alpha + |x - x_0|}(x - x_0)$\n- Real NVP; affine coupling layer; $f(x^{(2)}) = t(x^{(1)}) + x^{(2)}\\odot\\exp s(x^{(1)}) $\n- Masked Autoregressive Flow (MAF); $f(x_i) = (x_i - \\mu(x_{<i})) / \\exp(\\alpha(x_{<i}))$\n- Invertible 1x1 Convolution (Glow);\n- ActNorm; $f(x) = Wx + b$ where $W$ is diagonal and $b$ is a constant\n- Autoregressive Neural Spline Flow (NSF-AF); $f(x_i) = \\mathrm{RQS}_{\\theta(x_{<i})}(x_i)$\n- Coupling Neural Spline Flow (NSF-CL); $f(x^{(2)}) = \\mathrm{RQS}_{\\theta(x^{(1)})}(x^{(2)})$","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"8c27bd","input":"Thus, the final pdf of our distribution can be evaluated as \n\n$$p(z_k)=\\frac{p(z_0)}{\\Pi_{i=1}^k J_i}$$\n\nor, \n\n$$log(p(z_k))=log(p(z_0))-\\Sigma_{i=1}^klog(J_i)$$","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"8dfc0d","input":"**Problem:** ... but known pdf is changed at each point after each transformation $f$\n\n![](https://2.bp.blogspot.com/-1vyL7LpM1io/Wl-ghB0yOiI/AAAAAAAAHkM/_U94kuVeQpk22J5Mg0lbLK-EdMDkaQWggCLcBGAs/s1600/flow1.png)\n\n**Solution:** The Jacobian is exactly the factor how volume is changed at each point $$J_k=|\\frac{\\partial f_k}{\\partial z_k}|$$","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"f604d8","input":"### We can stack multiple transformations f\n\n<!-- ![](https://lilianweng.github.io/lil-log/assets/images/normalizing-flow.png) -->","pos":4,"type":"cell"}
{"id":0,"time":1596712539217,"type":"user"}
{"last_load":1596712538116,"type":"file"}