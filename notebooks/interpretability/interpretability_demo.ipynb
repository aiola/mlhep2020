{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Deep models interpretability\n",
    "This notebooks shows examples of approaches for interpretation of (pre-)trained model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "try:\n",
    "    import skimage\n",
    "except ModuleNotFoundError:\n",
    "    import subprocess as sp\n",
    "    result = sp.run(\n",
    "        ['pip3', 'install', 'scikit-image'],\n",
    "        stdout=sp.PIPE, stderr=sp.PIPE\n",
    "    )\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(result.stdout.decode('utf-8'))\n",
    "        print(result.stderr.decode('utf-8'))\n",
    "    \n",
    "    import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import urllib\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models\n",
    "\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def get_free_gpu():\n",
    "    from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlDeviceGetCount\n",
    "    nvmlInit()\n",
    "\n",
    "    return np.argmax([\n",
    "        nvmlDeviceGetMemoryInfo(nvmlDeviceGetHandleByIndex(i)).free\n",
    "        for i in range(nvmlDeviceGetCount())\n",
    "    ])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    cuda_id = get_free_gpu()\n",
    "    device = 'cuda:%d' % (get_free_gpu(), )\n",
    "    print('Selected %s' % (device, ))\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('WARNING: using cpu!')\n",
    "\n",
    "### please, don't remove the following line\n",
    "x = torch.tensor([1], dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# load imagenet class names\n",
    "import requests\n",
    "response = requests.get('https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json')\n",
    "class_labels = {int(key): value for key, (code, value) in response.json().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class_labels[897]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step I: manual explanation with smoothgrad\n",
    "\n",
    "Let us begin by implementing our own little explainer for [DenseNet121](https://arxiv.org/abs/1608.06993) pretrained on ImageNet.\n",
    "\n",
    "For the sake of simplicity, we're gonna rely on [SmoothGrad](https://arxiv.org/pdf/1706.03825.pdf) explainer - a simple average of gradients over noisy inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# model = torchvision.models.densenet121(pretrained=True).train(False).to(device)\n",
    "model = torchvision.models.resnet50(pretrained=True).train(False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "!wget -q https://cdn2.adrianflux.co.uk/wp-fluxposure/uploads/2014/08/no-7.jpg -O img.jpg\n",
    "\n",
    "image = resize(plt.imread('img.jpg')[..., :3] / 255.0, (224, 224))\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_tensor = torch.as_tensor(image, device=device, dtype=torch.float32)\n",
    "    image_tensor = image_tensor[None].permute(0, 3, 1, 2)\n",
    "    probs = torch.softmax(model(image_tensor), dim=-1)[0]\n",
    "    \n",
    "for i, class_ix in enumerate(probs.argsort(descending=True)[:10]):\n",
    "    print(f\"#{i + 1}: p={probs[class_ix].item():.3f}\\t{class_labels[class_ix.item()]} ({class_ix.item()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now let's implement SmoothGrad itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def explain_smoothgrad(image, class_ix, num_runs=32, sigma=0.1):\n",
    "    image_tensor = torch.as_tensor(image, device=device, dtype=torch.float32)\n",
    "    image_tensor = image_tensor[None].permute(0, 3, 1, 2).requires_grad_(True)\n",
    "    \n",
    "    # repeat image tensor several times, add different noise to each copy\n",
    "    repeated_image = image_tensor.repeat(num_runs, 1, 1, 1)\n",
    "    noisy_images = repeated_image + sigma * torch.randn_like(repeated_image)\n",
    "    \n",
    "    class_scores = model(noisy_images)[:, class_ix]\n",
    "    class_scores.sum().backward()  # backpropagate to image_tensor\n",
    "    return abs(image_tensor.grad).mean(dim=1)[0].cpu().numpy()\n",
    "\n",
    "for class_ix in 163, 717, 897:  # <-- insert your classes here, use numbers in (brackets)\n",
    "    plt.title(f'Explaining \"{class_labels[class_ix]}\"')\n",
    "    plt.imshow(explain_smoothgrad(image, class_ix), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "__Bonus round!__ Peter Higgs nobel prize photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "!wget -q https://i.pinimg.com/originals/3a/e1/83/3ae18369ab2e86be83e637ad702ec832.jpg -O img.jpg\n",
    "\n",
    "image = resize(plt.imread('img.jpg')[..., :3] / 255.0, (224, 224))\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_tensor = torch.as_tensor(image, device=device, dtype=torch.float32)\n",
    "    image_tensor = image_tensor[None].permute(0, 3, 1, 2)\n",
    "    probs = torch.softmax(model(image_tensor), dim=-1)[0]\n",
    "    \n",
    "for i, class_ix in enumerate(probs.argsort(descending=True)[:10]):\n",
    "    print(f\"#{i + 1}: p={probs[class_ix].item():.3f}\\t{class_labels[class_ix.item()]} ({class_ix.item()})\")\n",
    "    \n",
    "for class_ix in 834, 457, 861:\n",
    "    plt.title(f'Explaining \"{class_labels[class_ix]}\"')\n",
    "    plt.imshow(explain_smoothgrad(image, class_ix), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Part II: SHapley Additive exPlanations\n",
    "Now, let's try something heavier. The current state of the art in explaining model predictions is [SHAP](https://arxiv.org/abs/1705.07874): Shapley Additive Explanations.\n",
    "\n",
    "This method is based on [Shapley values](https://en.wikipedia.org/wiki/Shapley_value) - a game-theoretic concept that evaluates the contribution of individual players in a cooperative game. Except this time our \"players\" are input features and the \"game\" is predicting whichever output the model gave.\n",
    "\n",
    "Computing Shapley values naively requires $O(F!)$ time where F is the number of features. To make this computation more feasible, authors [proposed](https://arxiv.org/abs/1705.07874) several approximations, one of which relies on averaged gradients. This approximation also requires \"background\" data - other images similar to the ones in question that can be used as reference points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "try:\n",
    "    import shap\n",
    "except ModuleNotFoundError:\n",
    "    import subprocess as sp\n",
    "    result = sp.run(\n",
    "        ['pip3', 'install', 'shap'],\n",
    "        stdout=sp.PIPE, stderr=sp.PIPE\n",
    "    )\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(result.stdout.decode('utf-8'))\n",
    "        print(result.stderr.decode('utf-8'))\n",
    "    \n",
    "    import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "shap.initjs()\n",
    "\n",
    "# load \"background\" images - some 50 random images from ImageNet\n",
    "background, _ = shap.datasets.imagenet50()\n",
    "background = torch.as_tensor(background / 255.0, device=device, dtype=torch.float32)\n",
    "background = background.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "background.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# image to explain\n",
    "!wget -q https://i.pinimg.com/originals/32/da/5c/32da5c3314fcc5ebf1a7b7d1548fcb03.jpg -O img.jpg\n",
    "image = resize(plt.imread('img.jpg')[..., :3] / 255.0, (224, 224))\n",
    "image_tensor = torch.as_tensor(image[None], device=device, dtype=torch.float32).permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# explain and visualize. If you're not using a powerful V100 GPU, this may take up to 20 minutes.\n",
    "explainer = shap.GradientExplainer((model, model.layer1), background)\n",
    "shap_values, indices = explainer.shap_values(image_tensor, ranked_outputs=5, nsamples=200)\n",
    "shap_values = [np.transpose(values, (0, 2, 3, 1)) for values in shap_values]\n",
    "index_names = np.vectorize(lambda i: class_labels[i])(indices.cpu().numpy())\n",
    "shap.image_plot(shap_values, image_tensor.permute(0, 2, 3, 1).cpu().numpy(), index_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# obligatory physicist reference\n",
    "!wget -q https://images-na.ssl-images-amazon.com/images/I/51ArQaCkkZL._AC_.jpg -O img.jpg\n",
    "image = resize(plt.imread('img.jpg')[..., :3] / 255.0, (224, 224))\n",
    "image_tensor = torch.as_tensor(image[None], device=device, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "explainer = shap.GradientExplainer((model, model.layer1), background)\n",
    "shap_values, indices = explainer.shap_values(image_tensor, ranked_outputs=3, nsamples=200)\n",
    "shap_values = [np.transpose(values, (0, 2, 3, 1)) for values in shap_values]\n",
    "index_names = np.vectorize(lambda i: class_labels[i])(indices.cpu().numpy())\n",
    "shap.image_plot(shap_values, image_tensor.permute(0, 2, 3, 1).cpu().numpy(), index_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Part III: explaining classical machine learning models\n",
    "\n",
    "Finally, let's see how SHAP explainers can be applied to more conventional machine learning models like gradient boosting. \n",
    "\n",
    "Spoiler: exactly the same from a user's perspective. However, this time we're gonna use a different Shapley approximation implemented in TreeExplainer. For a full set of available explainers, take a look at their official [examples page](https://github.com/slundberg/shap/tree/master/notebooks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**NB: the following cells requires JavaScript support and should work in Jupyter (not JupyterLab or cocalc)**. \n",
    "In that case try running it on Google colab: https://colab.research.google.com/github/yandexdataschool/mlhep2020-assignments/blob/master/notebooks/interpretability/interpretability_demo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import catboost # pip install catboost\n",
    "\n",
    "X, y = shap.datasets.boston()\n",
    "ensemble = catboost.CatBoostRegressor(iterations=100, learning_rate=0.1)\n",
    "ensemble.fit(X, y, verbose=False, plot=False)\n",
    "explainer = shap.TreeExplainer(ensemble)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# visualize explanation of the first example\n",
    "shap.force_plot(explainer.expected_value, shap_values[0, :], X.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Explain training data: each *column* is a rotated plot from above, stacked for all training samples\n",
    "# (this plot is interactive, hover mouse to see feature names)\n",
    "shap.force_plot(explainer.expected_value, shap_values, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## References\n",
    "\n",
    "* SHAP explainer based on superpixels - [notebook](https://slundberg.github.io/shap/notebooks/ImageNet%20VGG16%20Model%20with%20Keras.html)\n",
    "* SHAP reference notebook - [view on github](https://github.com/slundberg/shap/tree/master/notebooks)\n",
    "* More various explainers in [ELI5](https://github.com/TeamHG-Memex/eli5)\n",
    "* Same notebook on Google [Colab](https://colab.research.google.com/github/yandexdataschool/mlhep2020-assignments/blob/master/notebooks/interpretability/interpretability_demo.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}