{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Seminar plan\n",
    "\n",
    "In this notebook we will:\n",
    "\n",
    "    1. Implement variational optimization:\n",
    "        - and find even more pitfalls in \"theoretically nicely working\" methods;\n",
    "    2. Implement Gumbel Softmax Trick:\n",
    "        - and train classifier with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import pyro\n",
    "from pyro import distributions as distrs\n",
    "import seaborn as sns\n",
    "import numpy.testing as np_testing\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "sns.set(font_scale=1.5, rc={'figure.figsize':(11.7, 8.27)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def sample(d: distrs.Distribution, num: int):\n",
    "    \"\"\"\n",
    "    Sample from d samples with Pyro reparameterization capabilities\n",
    "    \"\"\"\n",
    "    res = pyro.sample(\"dist\", d.expand([num]))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. Optimization of the synthetic function\n",
    "\n",
    "We are going to optimize this function:\n",
    "\n",
    "$$ f(x) = \\log ((x - 0.35)^2 + 1) - 0.15 \\exp(-10 |x - 0.8|)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def fun_from_lecture(x):\n",
    "    \"\"\"\n",
    "    Some unknown synthetic function from the lecture\n",
    "    \"\"\"\n",
    "    return (torch.log1p((x - 0.35) ** 2) - 0.15 * torch.exp(-10 * torch.abs(x - 0.8))).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "x = torch.linspace(0., 1., 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(x, fun_from_lecture(x))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Some unknown synthetic function from the lecture')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def get_parameters_for_normal_dsitribution(mu=0., sigma=1., dim=2):\n",
    "    \"\"\"\n",
    "    Creates trainable parameters for the normal distribution\n",
    "    \"\"\"\n",
    "    train_mu = torch.Tensor(dim * [mu]).float().requires_grad_(True)\n",
    "    train_sigma = (torch.eye(dim) * sigma).float().requires_grad_(True)\n",
    "    return train_mu, train_sigma\n",
    "\n",
    "def create_normal_distr(mu, sigma):\n",
    "    \"\"\"\n",
    "    Given mu and sigma returns MultivariateNormal\n",
    "    \"\"\"\n",
    "    return distrs.MultivariateNormal(mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "mu, sigma = get_parameters_for_normal_dsitribution(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.1 Estimation of the variational bound\n",
    "\n",
    "$\\DeclareMathOperator*{\\myE}{\\mathop{\\mathbb{E}}}$\n",
    "\n",
    "##### In theory:\n",
    "\n",
    "$$J(\\psi) = \\myE_{\\theta \\sim P(\\cdot \\,\\mid\\, \\psi)} f(\\theta) \\to \\min_\\psi$$\n",
    "\n",
    "\n",
    "##### In practice:\n",
    "\n",
    "$$J(\\psi) = \\frac{1}{N} \\sum\\limits_{\\theta \\sim P(\\cdot \\,\\mid\\, \\psi)} f(\\theta) \\to \\min_\\psi,$$\n",
    "\n",
    "where the choice of $N$ depends only on your resources and availability of $f(\\cdot)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16364abafa30ad91c77a42e7c9402a3d",
     "grade": false,
     "grade_id": "cell-074854468043e32c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
   ],
   "source": [
    "from typing import Callable, List\n",
    "def estimage_variational_bound(fun: Callable, create_dist: Callable, parameters: List[torch.Tensor], num_samples=5):\n",
    "    \"\"\"\n",
    "    fun: function for which you want to estimate variational bound \n",
    "    create_dist: function that takes list of parameters and returns pyro distribution\n",
    "    parameters: parameters of the distribution, that you will pass to create dist function\n",
    "    num_samples: number of samples used to evaluate var bound\n",
    "    \n",
    "    var_bound: \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return var_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "estimage_variational_bound",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
   ],
   "source": [
    "mu = torch.tensor([0.])\n",
    "sigma = torch.tensor([[1.]])\n",
    "num_samples = 100\n",
    "res = [estimage_variational_bound(\n",
    "    fun=fun_from_lecture,\n",
    "    create_dist=create_normal_distr,\n",
    "    parameters=[mu, sigma],\n",
    "    num_samples=num_samples\n",
    ") for _ in range(100)]\n",
    "\n",
    "assert 0.56 - 0.06 * 7 < torch.stack(res).mean() < 0.56 + 0.06 * 7, \"It should be impossible...\"\n",
    "assert 0.06 - 0.005 * 7 < torch.stack(res).std() < 0.06 + 0.005 * 7, \"That's also should be impossible, it's 7 sigma!\"\n",
    "\n",
    "mu = torch.tensor([0.8])\n",
    "sigma = torch.tensor([[0.01]])\n",
    "num_samples = 100000000\n",
    "res = estimage_variational_bound(\n",
    "    fun=fun_from_lecture,\n",
    "    create_dist=create_normal_distr,\n",
    "    parameters=[mu, sigma],\n",
    "    num_samples=num_samples\n",
    ")\n",
    "np_testing.assert_approx_equal(actual=res.item(), desired=0.1114, significant=2)\n",
    "\n",
    "\n",
    "mu = torch.tensor([0.3])\n",
    "sigma = torch.tensor([[0.02]])\n",
    "num_samples = 100000000\n",
    "res = estimage_variational_bound(\n",
    "    fun=fun_from_lecture,\n",
    "    create_dist=create_normal_distr,\n",
    "    parameters=[mu, sigma],\n",
    "    num_samples=num_samples\n",
    ")\n",
    "np_testing.assert_approx_equal(actual=res.item(), desired=0.01907, significant=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Let's plot some variational bounds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from collections import defaultdict\n",
    "num_samples = 1000\n",
    "\n",
    "sigma_plots = defaultdict(list)\n",
    "mus = torch.linspace(0., 1., 100).view(-1, 1)\n",
    "\n",
    "for sigma in tqdm(torch.tensor([0.01, 0.05, 0.1, 0.2]).view(-1, 1, 1).pow(2)):\n",
    "    for mu in mus:\n",
    "        sigma_plots[sigma.sqrt().item()].append(\n",
    "            estimage_variational_bound(\n",
    "                fun=fun_from_lecture,\n",
    "                create_dist=create_normal_distr,\n",
    "                parameters=[mu, sigma],\n",
    "                num_samples=num_samples\n",
    "            )\n",
    "        )\n",
    "for sigma in sigma_plots:\n",
    "    sigma_plots[sigma] = torch.stack(sigma_plots[sigma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "for sigma in sigma_plots:\n",
    "    plt.plot(mus, sigma_plots[sigma], label=r\"$\\sigma={:.4}$\".format(sigma))\n",
    "plt.legend()\n",
    "plt.plot(x, fun_from_lecture(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Moar samples](https://raw.githubusercontent.com/SchattenGenie/pic-storage/master/moar_samples.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.2 Now we are going to estimate the gradient of the variational bound\n",
    "\n",
    "In theory:\n",
    "\n",
    "$$\\nabla_\\psi J(\\psi) = \\myE_{\\theta \\sim P(\\cdot\\mid\\psi)} f(\\theta)\\; \\nabla_\\psi \\log P(\\theta\\mid\\psi)$$\n",
    "\n",
    "In practice:\n",
    "\n",
    "$$\\nabla_\\psi J(\\psi) = \\frac{1}{N} \\sum\\limits_{\\theta \\sim P(\\cdot \\,\\mid\\, \\psi)}  f(\\theta)\\; \\nabla_\\psi \\log P(\\theta\\mid\\psi)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a83336ca07eea07e32ae10598a6fc297",
     "grade": false,
     "grade_id": "cell-c25bb6d709bac8f5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
   ],
   "source": [
    "from torch import autograd\n",
    "\n",
    "def estimage_variational_grad(fun: Callable, create_dist: Callable, parameters: List[torch.Tensor], num_samples=5):\n",
    "    \"\"\"\n",
    "    fun: function for which you want to estimate variational bound \n",
    "    create_dist: function that takes list of parameters and returns pyro distribution\n",
    "    parameters: parameters of the distribution, that you will pass to create dist function\n",
    "    num_samples: number of samples used to evaluate var bound\n",
    "    \n",
    "    grads: list of torch tensors with the same shape as \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "estimage_variational_grad",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
   ],
   "source": [
    "num_samples = 100000000\n",
    "\n",
    "mu = torch.tensor([0.8])\n",
    "sigma = torch.tensor([[0.01]])\n",
    "grad = estimage_variational_grad(\n",
    "    fun=fun_from_lecture,\n",
    "    create_dist=create_normal_distr,\n",
    "    parameters=[mu, sigma],\n",
    "    num_samples=num_samples\n",
    ")\n",
    "np_testing.assert_approx_equal(actual=grad[0].item(), desired=0.73411, significant=2)\n",
    "np_testing.assert_approx_equal(actual=grad[1].item(), desired=2.568, significant=2)\n",
    "\n",
    "\n",
    "mu = torch.tensor([0.2])\n",
    "sigma = torch.tensor([[0.04]])\n",
    "grad = estimage_variational_grad(\n",
    "    fun=fun_from_lecture,\n",
    "    create_dist=create_normal_distr,\n",
    "    parameters=[mu, sigma],\n",
    "    num_samples=num_samples\n",
    ")\n",
    "np_testing.assert_approx_equal(actual=grad[0].item(), desired=-0.28668, significant=2)\n",
    "np_testing.assert_approx_equal(actual=grad[1].item(), desired=0.75752, significant=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.3 Optimization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Do not look here, just some data for some plots..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "num_samples = 100000\n",
    "\n",
    "mu_grid = np.linspace(0, 1, num=21)\n",
    "sigma_grid = np.linspace(0, 0.5, num=21)[1:]\n",
    "\n",
    "Mu, Sigma = np.meshgrid(mu_grid, sigma_grid,)\n",
    "\n",
    "dJ_dmu = np.zeros(shape=(mu_grid.shape[0], sigma_grid.shape[0]))\n",
    "dJ_dsigma = np.zeros(shape=(mu_grid.shape[0], sigma_grid.shape[0]))\n",
    "\n",
    "for i, mu in tqdm(enumerate(mu_grid)):\n",
    "    for j, sigma in enumerate(sigma_grid):\n",
    "        grads = estimage_variational_grad(\n",
    "            fun=fun_from_lecture,\n",
    "            create_dist=create_normal_distr,\n",
    "            parameters=[torch.tensor(mu).view(1), torch.tensor(sigma).pow(2).view(1, 1)],\n",
    "            num_samples=num_samples\n",
    "        )\n",
    "        dJ_dmu[i, j], dJ_dsigma[i, j] = grads[0].item(), grads[1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def plot_optimization_path():\n",
    "    fig = plt.figure(figsize=(9, 9), constrained_layout=True, dpi=100)\n",
    "    gs = fig.add_gridspec(2, 1, height_ratios=[1, 2])\n",
    "\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    for sigma in sigma_plots:\n",
    "        plt.plot(mus, sigma_plots[sigma], label=r\"$\\sigma={:.4}$\".format(sigma))\n",
    "\n",
    "    fig.add_subplot(gs[1, 0], sharex=ax1)\n",
    "\n",
    "    plt.vlines([0.35, 0.8], ymin=0, ymax=sigma_grid[-1], lw=2, linestyles='--', color=plt.cm.tab10(1), alpha=0.75)\n",
    "    plt.quiver(mu_grid, sigma_grid, -dJ_dmu.T, -dJ_dsigma.T, angles='xy')\n",
    "\n",
    "    plt.ylim([0, sigma_grid[-1]])\n",
    "    plt.xlim([mu_grid[0], mu_grid[-1]])\n",
    "\n",
    "    plt.xlabel('$\\\\mu$');\n",
    "    plt.ylabel('$\\\\sigma$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Optimization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. Initialize parameters $\\mu$ and $\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "mu, sigma = get_parameters_for_normal_dsitribution(mu=0.95, sigma=0.5, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "2. Get your favorite optimizer ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "opt = torch.optim.SGD([mu, sigma], lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "3. Optimization start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "epochs = 1000\n",
    "num_samples = 100\n",
    "eps = 1e-7\n",
    "hist = defaultdict(list)\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    sigma.data.clamp_(eps)\n",
    "    grad_mu, grad_sigma = estimage_variational_grad(\n",
    "        fun=fun_from_lecture, \n",
    "        create_dist=create_normal_distr, \n",
    "        parameters=[mu, sigma], \n",
    "        num_samples=num_samples\n",
    "    )\n",
    "    hist[\"mu\"].append(mu.item())\n",
    "    hist[\"sigma\"].append(sigma.item())\n",
    "    \n",
    "    hist[\"grad_mu\"].append(grad_mu.item())\n",
    "    hist[\"grad_sigma\"].append(grad_sigma.item())\n",
    "\n",
    "    hist[\"fun\"].append(\n",
    "        estimage_variational_bound(\n",
    "            fun=fun_from_lecture, \n",
    "            create_dist=create_normal_distr, \n",
    "            parameters=[mu, sigma], \n",
    "            num_samples=100000\n",
    "        ).item()\n",
    "    )\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    # GOSH! We didn't backprop between `opt.zero_grad()` and `opt.step()`\n",
    "    mu.grad = grad_mu.data\n",
    "    sigma.grad = grad_sigma.data\n",
    "    opt.step()\n",
    "    sigma.data.clamp_(eps)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        fig = plt.figure(figsize=(9, 9), constrained_layout=True, dpi=100)\n",
    "        gs = fig.add_gridspec(2, 1, height_ratios=[1, 2])\n",
    "\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        for sigma_tmp in sigma_plots:\n",
    "            plt.plot(mus, sigma_plots[sigma_tmp], label=r\"$\\sigma={:.4}$\".format(sigma_tmp))\n",
    "        plt.scatter(hist['mu'], fun_from_lecture(torch.tensor(hist['mu'])), c='r')\n",
    "\n",
    "        fig.add_subplot(gs[1, 0], sharex=ax1)\n",
    "\n",
    "        plt.vlines([0.35, 0.8], ymin=0, ymax=sigma_grid[-1], lw=2, linestyles='--', color=plt.cm.tab10(1), alpha=0.75)\n",
    "        plt.quiver(mu_grid, sigma_grid, -dJ_dmu.T, -dJ_dsigma.T, angles='xy')\n",
    "        plt.scatter(hist['mu'], hist['sigma'], c='r')\n",
    "\n",
    "        plt.ylim([0, sigma_grid[-1]])\n",
    "        plt.xlim([mu_grid[0], mu_grid[-1]])\n",
    "\n",
    "        plt.xlabel('$\\\\mu$');\n",
    "        plt.ylabel('$\\\\sigma$');\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 8), dpi=100)\n",
    "axes[0].plot(hist['mu'], label=r'$\\mu$')\n",
    "axes[0].plot(hist['sigma'], label=r'$\\sigma$')\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(hist['grad_mu'], label=r'$grad \\mu$')\n",
    "axes[1].plot(hist['grad_sigma'], label=r'$grad \\sigma$')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### If you observe something strange during optimization try to change parameter in `sigma.data.clamp_`\n",
    "\n",
    "### Lesson: gradients are not stable\n",
    "\n",
    "$$\\nabla_{\\mu} J(\\mu, \\sigma) = f(x) \\frac{x - \\mu}{\\sigma^2} \\propto \\frac{1}{\\sigma}$$\n",
    "\n",
    "$$\\nabla_{\\sigma} J(\\mu, \\sigma) = f(x) \\frac{(x - \\mu)^2 - \\sigma^2}{\\sigma^2} \\propto \\frac{1}{\\sigma}$$\n",
    "\n",
    "So it is possible that gradients are going to oscillate.\n",
    "\n",
    "![Gradient instability](https://raw.githubusercontent.com/SchattenGenie/pic-storage/master/gradient_instability.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2. Gumbel Softmax Trick\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "![](https://miro.medium.com/max/640/1*1vCjxSiR1DArqH208HcLcQ.png)\n",
    "\n",
    "Many objects we want to learn are fundamentally discrete -- words, object classes, musical notes. We can think in this categories, but Neural Network is not able.\n",
    "\n",
    "If we want neural network to be able to operate with objects and concepts, it should be able to be work with discrete variable.\n",
    "\n",
    "#### Note: necessary but not sufficient condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### One of the ways to teach network discrete variables is Gumbel Softmax\n",
    "\n",
    "\n",
    "![Pic](https://raw.githubusercontent.com/SchattenGenie/pic-storage/master/humbel_meme.jpg)\n",
    "\n",
    "$$y_i = \\frac{ \\exp((\\log \\pi_i + G_i) / \\tau) }{\\sum\\limits_j \\exp((\\log \\pi_j + G_j) / \\tau)},~~~G_i = - \\log (-\\log u_i),~~~u_i \\sim U[0, 1],$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### To demonstrate this technique in practice, we will train classifier on the MNIST with the discrete layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.1 Setting up all neccessery functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "afef16f8e979222cc71a98f0ecb637e1",
     "grade": false,
     "grade_id": "cell-0558d36b4dcdb159",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
   ],
   "source": [
    "def sample_gumbel(shape, eps=1e-20): \n",
    "    \"\"\"\n",
    "    shape: tuple of ints, i.e. (1, 2) or (54, 7, 3)\n",
    "    \n",
    "    Sample from Gumbel(0, 1)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "sample_gumbel",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
   ],
   "source": [
    "assert sample_gumbel((1, 2, 3)).shape == (1, 2, 3)\n",
    "assert sample_gumbel((10, 100)).shape == (10, 100)\n",
    "m = (-(-sample_gumbel((1000, 1000))).exp()).exp()\n",
    "np_testing.assert_approx_equal(m.mean(), 0.5, significant=2)\n",
    "assert (-(-sample_gumbel((1000, 1000))).exp()).exp().max() <= 1.\n",
    "assert (-(-sample_gumbel((1000, 1000))).exp()).exp().min() >= 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1efa3211d823ba220d339a240bc1f17",
     "grade": false,
     "grade_id": "cell-b91d2cdf85a646f5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
   ],
   "source": [
    "def gumbel_softmax_sample(logits, temperature): \n",
    "    \"\"\"\n",
    "    logits: [batch_size, num_distributions]torch.Tensor that represents unnormalized log-probs \n",
    "             of the batch_size categorical distributions\n",
    "    temperature: torch.Tensor scalar, temperature of the Gumbel SoftMax Distribution\n",
    "    \n",
    "    Draw a sample from the Gumbel-Softmax distribution\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "gumbel_softmax_sample",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
   ],
   "source": [
    "logits = torch.tensor([[1, 2, 8, 1, 2]]).float()\n",
    "temperature = torch.tensor(1e10)\n",
    "np_testing.assert_allclose(\n",
    "    gumbel_softmax_sample(logits, temperature).numpy(), \n",
    "    np.array([[0.2000, 0.2000, 0.2000, 0.2000, 0.2000]]),\n",
    "    rtol=1e-3,\n",
    "    atol=1e-3\n",
    ")\n",
    "\n",
    "\n",
    "temperature = torch.tensor(1e-20)\n",
    "np_testing.assert_allclose(\n",
    "    gumbel_softmax_sample(logits, temperature).numpy(), \n",
    "    np.array([[0., 0., 1., 0., 0.]]),\n",
    "    rtol=1e-3,\n",
    "    atol=1e-3\n",
    ")\n",
    "\n",
    "temperature = torch.tensor(1.)\n",
    "np_testing.assert_allclose(\n",
    "    gumbel_softmax_sample(logits, temperature).sum().numpy(), \n",
    "    1.,\n",
    "    rtol=1e-3,\n",
    "    atol=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da7260c8fcab5b5b99deb7dc59166ab5",
     "grade": false,
     "grade_id": "cell-c6e12c6d0df98b57",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
   ],
   "source": [
    "def gumbel_softmax(logits, temperature, hard=False):\n",
    "    \"\"\"\n",
    "    Sample from the Gumbel-Softmax distribution and optionally discretize.\n",
    "    \n",
    "    logits: [batch_size, num_distributions] unnormalized log-probs\n",
    "    temperature: non-negative scalar\n",
    "    hard: if True, take argmax, but differentiate w.r.t. soft sample y\n",
    "    \n",
    "    Returns:\n",
    "    [batch_size, num_distributions] sample from the Gumbel-Softmax distribution.\n",
    "    If hard=True, then the returned sample will be one-hot, otherwise it will\n",
    "    be a probabilitiy distribution that sums to 1 across classes\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "gumbel_softmax",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
   ],
   "source": [
    "logits = torch.tensor([[1, 2, 8, 1, 2]]).float()\n",
    "temperature = torch.tensor(1e10)\n",
    "np_testing.assert_allclose(\n",
    "    gumbel_softmax(logits, temperature).numpy(), \n",
    "    np.array([[0.2000, 0.2000, 0.2000, 0.2000, 0.2000]]),\n",
    "    rtol=1e-3,\n",
    "    atol=1e-3\n",
    ")\n",
    "\n",
    "\n",
    "temperature = torch.tensor(1e-20)\n",
    "np_testing.assert_allclose(\n",
    "    gumbel_softmax(logits, temperature).numpy(), \n",
    "    np.array([[0., 0., 1., 0., 0.]]),\n",
    "    rtol=1e-3,\n",
    "    atol=1e-3\n",
    ")\n",
    "\n",
    "temperature = torch.tensor(1.)\n",
    "np_testing.assert_allclose(\n",
    "    gumbel_softmax(logits, temperature).sum().numpy(), \n",
    "    1.,\n",
    "    rtol=1e-3,\n",
    "    atol=1e-3\n",
    ")\n",
    "\n",
    "\n",
    "temperature = torch.tensor(1e10)\n",
    "np_testing.assert_allclose(\n",
    "    gumbel_softmax(logits, temperature, hard=True).numpy().max(), \n",
    "    1.,\n",
    "    rtol=1e-3,\n",
    "    atol=1e-3\n",
    ")\n",
    "\n",
    "\n",
    "temperature = torch.tensor(1e-20)\n",
    "np_testing.assert_allclose(\n",
    "    gumbel_softmax(logits, temperature).numpy(), \n",
    "    np.array([[0., 0., 1., 0., 0.]]),\n",
    "    rtol=1e-3,\n",
    "    atol=1e-3\n",
    ")\n",
    "\n",
    "temperature = torch.tensor(1.)\n",
    "np_testing.assert_allclose(\n",
    "    gumbel_softmax(logits, temperature).sum().numpy(), \n",
    "    1.,\n",
    "    rtol=1e-3,\n",
    "    atol=1e-3\n",
    ")\n",
    "\n",
    "logits = torch.tensor([[1, 2, 8, 1, 2]]).float().requires_grad_(True)\n",
    "assert gumbel_softmax(logits, temperature).requires_grad == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## N.B. Small lecture follow-up \n",
    "\n",
    "In the lecture I wrote:\n",
    "\n",
    "\n",
    "During forward pass:\n",
    "\n",
    "$\\newcommand{\\Argmax}{\\mathop{{\\rm Arg\\,max}}}$\n",
    "$$z = \\textrm{one-hot}(\\Argmax \\{ y_i \\}),$$\n",
    "\n",
    "During backward pass:\n",
    "\n",
    "$$\\frac{dy}{d\\pi} \\rightarrow \\frac{dz}{d \\pi},~~~\\tau \\rightarrow 0$$\n",
    "\n",
    "\n",
    "##### However, there is one thing to note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "logits = torch.tensor([[1, 2, 4, 1, 2]]).float().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### With temperature, soft Gumbel Softmax Trick:\n",
    "\n",
    "##### You can use soft version in networks and functions that are able to work with RELAXED discrete input variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### During forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "temperature = torch.tensor(1.)\n",
    "output = gumbel_softmax(logits, temperature, hard=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### During backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "autograd.grad(output.sin().sum(), logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### With temperature, hard Gumbel Softmax Trick:\n",
    "\n",
    "\n",
    "##### You should use hard version in functions that require discrete function as an input ONLY discrete input variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### During forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "temperature = torch.tensor(1.)\n",
    "output = gumbel_softmax(logits, temperature, hard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### During backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "autograd.grad(output.sin().sum(), logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.2 Now we are all set to train a small classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class GumbelLayer(nn.Module):\n",
    "    def __init__(self, in_features, num_distributions=20, num_classes=10, temperature=5., hard=False, flatten=True):\n",
    "        super(GumbelLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, num_distributions * num_classes)\n",
    "        self.num_classes = num_classes\n",
    "        self.num_distributions = num_distributions\n",
    "        self.temperature = temperature\n",
    "        self.hard = hard\n",
    "        self.flatten = flatten\n",
    "        print(\"Output size of GumbelLayer is equal to: {}\".format(num_distributions * num_classes))\n",
    "\n",
    "    def forward(self, input):\n",
    "        logits_y = self.linear(input).view(-1, self.num_classes)\n",
    "        y = gumbel_softmax(logits_y, self.temperature, hard=self.hard).view(-1, self.num_distributions * self.num_classes)\n",
    "        if not self.flatten:\n",
    "            y = y.view(-1, self.num_distributions, self.num_classes)\n",
    "        return y\n",
    "\n",
    "    def set_hard(self, hard):\n",
    "        self.hard = hard\n",
    "        \n",
    "    def set_temperature(self, temperature):\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def set_flatten(self, flatten):\n",
    "        self.flatten = flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def get_free_gpu():\n",
    "    from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlDeviceGetCount\n",
    "    nvmlInit()\n",
    "\n",
    "    return np.argmax([\n",
    "        nvmlDeviceGetMemoryInfo(nvmlDeviceGetHandleByIndex(i)).free\n",
    "        for i in range(nvmlDeviceGetCount())\n",
    "    ])\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    cuda_id = get_free_gpu()\n",
    "    device = 'cuda:%d' % (get_free_gpu(), )\n",
    "    print('Selected %s' % (device, ))\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('WARNING: using cpu!')\n",
    "\n",
    "### please, don't remove the following line\n",
    "x = torch.tensor([1], dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Building our tiny nice classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "classifier = torch.nn.Sequential(\n",
    "    nn.Linear(784, 512),\n",
    "    nn.Tanh(),\n",
    "    GumbelLayer(512, num_distributions=1, num_classes=10, temperature=5., hard=False),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(10, 64),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(64, 10),\n",
    "    nn.LogSoftmax(dim=-1)\n",
    ").to(device)\n",
    "\n",
    "opt = torch.optim.Adam(params=classifier.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "# Or was it ../../data?..\n",
    "train_dataset = datasets.MNIST('../../../share', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128)\n",
    "\n",
    "test_dataset = datasets.MNIST('../../../share', train=True, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "temperature = 1.\n",
    "hist = defaultdict(list)\n",
    "\n",
    "for epoch in tqdm(range(3)):\n",
    "    print(\"temperature={}\".format(temperature))\n",
    "    # temperature annealing\n",
    "    temperature = 1. / np.log(2. + epoch)\n",
    "    classifier[2].set_temperature(temperature)\n",
    "    \n",
    "    for X, target in tqdm(train_loader):\n",
    "        X = X.view(-1, 784).to(device)\n",
    "        target = target.to(device)\n",
    "        preds = classifier(X)\n",
    "        \n",
    "        loss = nn.NLLLoss()(preds, target)\n",
    "    \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    loss_test = 0.\n",
    "    accuracy_test = 0.\n",
    "    for X, target in tqdm(test_loader):\n",
    "        X = X.view(-1, 784).to(device)\n",
    "        target = target.to(device)\n",
    "        preds = classifier(X)\n",
    "        accuracy_test += (preds.argmax(dim=1) == target).float().mean().item() / len(test_loader)\n",
    "        loss_test += nn.NLLLoss()(preds, target).item() / len(test_loader)\n",
    "    \n",
    "    hist['accuracy'].append(accuracy_test)\n",
    "    hist['loss'].append(loss_test)\n",
    "    clear_output(wait=True)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 7))\n",
    "    axes[0].plot(np.arange(1 + epoch), hist['loss'], label=\"Loss\")\n",
    "    axes[0].legend()\n",
    "    axes[1].plot(np.arange(1 + epoch), hist['accuracy'], label=\"Accuracy\")\n",
    "    axes[1].legend()\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Let's chop the head of the classifier!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "discrete_net = nn.Sequential(*list(classifier.children())[:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "discrete_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "discrete_net = discrete_net.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Try to play with `hard` parameter and `temperature` of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "discrete_net[-1].set_hard(False)\n",
    "discrete_net[-1].set_flatten(False)\n",
    "discrete_net[-1].set_temperature(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "X_discrete = discrete_net(test_dataset.data.view(-1, 784).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "targets = test_dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "X_discrete[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Even without annealing of $\\tau$ to zero Gumbel predicts almost discrete variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(X_discrete[0].detach().cpu().numpy())\n",
    "plt.title('Number={}'.format(targets[0]))\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(X_discrete[1].detach().cpu().numpy())\n",
    "plt.title('Number={}'.format(targets[1]))\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(X_discrete[11].detach().cpu().numpy())\n",
    "plt.title('Number={}'.format(targets[11]))\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Let's calculate some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "stats = defaultdict(list)\n",
    "for i in range(len(X_discrete)):\n",
    "    stats[targets[i].item()].append(X_discrete[i].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "stats.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "mean_discrete_variable = []\n",
    "std_discrete_variable = []\n",
    "for num in range(10):\n",
    "    mean_discrete_variable.append(np.concatenate(stats[num]).mean(axis=0))\n",
    "    std_discrete_variable.append(np.concatenate(stats[num]).std(axis=0) / np.sqrt(len(stats[num])))\n",
    "mean_discrete_variable = np.array(mean_discrete_variable)\n",
    "std_discrete_variable = np.array(std_discrete_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import pandas as pd\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df = pd.DataFrame(mean_discrete_variable)\n",
    "corr = df.corr()\n",
    "\n",
    "mask = np.tril(np.ones_like(corr, dtype=np.bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Let's plot for each number distribution among discrete variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "fig, ax = plt.subplots(5, 2, sharex=True, figsize=(20, 22))\n",
    "for i in range(10):\n",
    "    ax[i % 5][i // 5].bar(np.arange(mean_discrete_variable.shape[1]), mean_discrete_variable[i], yerr=std_discrete_variable[i], align='center', alpha=0.5, ecolor='black', capsize=10)\n",
    "    ax[i % 5][i // 5].set_title(\"Number={}\".format(i))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}