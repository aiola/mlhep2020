{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Black-Box optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import os\n",
    "\n",
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "data = np.load('../../../share/SUSY-small.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_all, y_all = data[:, 1:], data[:, 0]\n",
    "\n",
    "X_train, X_rest, y_train, y_rest = train_test_split(X_all, y_all, test_size=0.5)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_rest, y_rest, test_size=0.5)\n",
    "\n",
    "print('Train     :', X_train.shape, y_train.shape)\n",
    "print('Validation:', X_val.shape, y_val.shape)\n",
    "print('Test      :', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def plot(f, history=None, trajectory=False):\n",
    "    xs = np.linspace(-0.5, 2.5, num=250)\n",
    "    X, Y = np.meshgrid(xs, xs)\n",
    "\n",
    "    Z = np.stack([X.reshape(-1), Y.reshape(-1)], axis=1)\n",
    "    F = f(Z[:, 0], Z[:, 1]).reshape(xs.shape[0], xs.shape[0])\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.ylim([-0.5, 2.5])\n",
    "    plt.xlim([-0.5, 2.5])\n",
    "\n",
    "    plt.contour(\n",
    "        xs, xs, F,\n",
    "        levels=np.linspace(0, 4, num=20),\n",
    "        colors=[plt.cm.tab10(0)]\n",
    "    )\n",
    "    plt.scatter([1], [1], marker='*', s=500, color=plt.cm.tab10(3), zorder=5)\n",
    "    \n",
    "    if history is not None:\n",
    "        plt.scatter(history[:, 0], history[:, 1], color=plt.cm.tab10(1), s=100)\n",
    "        if trajectory:\n",
    "            plt.plot(history[:, 0], history[:, 1], color=plt.cm.tab10(1), lw=3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The first objective function\n",
    "\n",
    "The first objective function is one of the most popular benchmark functions --- the Resenbrock function.\n",
    "We slightly modify it by applying $\\chi \\to \\log (1 + \\chi)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def log_resenbrock(x1, x2):\n",
    "    return np.log1p(\n",
    "        (1 - x1) ** 2 + 1 * (x2 - x1 ** 2) ** 2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "plot(log_resenbrock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The second objective function\n",
    "\n",
    "One of use-cases for black-box optimization is hyper-parameter tuning.\n",
    "Here, we measure quality of XGBoost as a function of two continous parameters --- learning rate and regularization coefficient.\n",
    "The dataset is a small version of the [SUSY](https://archive.ics.uci.edu/ml/datasets/SUSY) dataset (loaded above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### NOTE, due to problems with XGBoost on cocalc, the objective function was replaced\n",
    "### with the one based on sklearn's GradientBoostingClassifier.\n",
    "### Unfortunately, there is not equivalent of reg_lambda in GradientBoostingClassifier.\n",
    "### Thus, the second dimension (log_reg_lambda) is not used at all.\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def eval_xgb_quality(log_learning_rate, log_reg_lambda):\n",
    "    ### NOTE: added -2 to log_learning_rate and log_reg_lambda\n",
    "    learning_rate = np.exp(log_learning_rate - 2)\n",
    "\n",
    "    clf = GradientBoostingClassifier(\n",
    "        learning_rate=learning_rate,\n",
    "        #ccp_alpha=reg_lambda,\n",
    "\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "\n",
    "        subsample=1e-1,\n",
    "        random_state=111\n",
    "    )\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict_proba(X_val)\n",
    "    \n",
    "    return 1 - accuracy_score(y_val, predictions[:, 1] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "%%time\n",
    "\n",
    "### baseline quality is for comparison\n",
    "baseline_XGB = eval_xgb_quality(0, 0)\n",
    "print('XGBoost baseline: %.3lf' % (baseline_XGB,) )\n",
    "print('%.3lf' % (eval_xgb_quality(-1, -2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 1: Random Search\n",
    "\n",
    "- implement sampling for Random Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc0baf2e6597cb779eb0acdc5313e949",
     "grade": false,
     "grade_id": "RS",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
   ],
   "source": [
    "def random_search(f, bounds, n_evaluations=20, progress=lambda x: x, seed=111222):\n",
    "    bounds = np.array(bounds)\n",
    "    \n",
    "    history_x = []\n",
    "    history_f = []\n",
    "    \n",
    "    rng = np.random.RandomState(seed=seed)\n",
    "    ### rng has the same methods as np.random,\n",
    "    ### e.g., rng.uniform - np.random.uniform\n",
    "    \n",
    "    for _ in progress(range(n_evaluations)):\n",
    "        u = rng.uniform(0, 1, size=bounds.shape[0])\n",
    "        x = u * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n",
    "\n",
    "        value = f(*x)\n",
    "        \n",
    "        history_x.append(x)\n",
    "        history_f.append(value)\n",
    "    \n",
    "    best = np.argmin(history_f)\n",
    "    history_x = np.array(history_x)\n",
    "    \n",
    "    return history_x[best], history_f[best], history_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "x, _, history = random_search(\n",
    "    log_resenbrock, \n",
    "    bounds=[(-0.5, 2.5), (-0.5, 2.5)],\n",
    "    progress=tqdm,\n",
    "    n_evaluations=1000\n",
    ")\n",
    "\n",
    "assert log_resenbrock(x[0], x[1]) < 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "plot(log_resenbrock, history, trajectory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "RS-XGB",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
   ],
   "source": [
    "x, error, history = random_search(eval_xgb_quality, bounds=[(-3, 3), (-3, 3)], progress=tqdm)\n",
    "\n",
    "test_quality = eval_xgb_quality(*x)\n",
    "\n",
    "assert test_quality < baseline_XGB, 'Looks like the optimization algorithm did not improved over baseline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 2: numerical gradient estimation\n",
    "\n",
    "- implement numerical gradient estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71afe0903229116abb47c85d2b7d3525",
     "grade": false,
     "grade_id": "NG",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
   ],
   "source": [
    "def numerical_gradient(f, x0, learning_rate=1e-1, h=1e-2, n_evaluations=20, progress=lambda x: x):\n",
    "    n_iters = (n_evaluations - 1) // (1 + x0.shape[0])\n",
    "    current_x = x0\n",
    "    \n",
    "    history_x = [current_x]\n",
    "    \n",
    "    for _ in progress(range(n_iters)):\n",
    "        f_current = f(*current_x)\n",
    "        \n",
    "        df = np.zeros(shape=current_x.shape)\n",
    "        \n",
    "        for j in range(current_x.shape[0]):\n",
    "            x = np.copy(current_x)\n",
    "            x[j] = x[j] - h\n",
    "            \n",
    "            df[j] = f_current - f(*x)\n",
    "        \n",
    "        grad = df / h\n",
    "        \n",
    "        current_x = current_x - learning_rate * grad\n",
    "        \n",
    "        history_x.append(np.copy(current_x))\n",
    "    \n",
    "    return current_x, f(*current_x), np.array(history_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "x, _, history = numerical_gradient(\n",
    "    log_resenbrock,\n",
    "    x0=np.array([0.5, 0.5]),\n",
    "    progress=tqdm,\n",
    "    \n",
    "    n_evaluations=2500\n",
    ")\n",
    "\n",
    "assert np.all(np.abs(x - 1) < 1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "plot(log_resenbrock, history, trajectory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "NG-XGB",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
   ],
   "source": [
    "class Function(object):\n",
    "    def __init__(self, f, max_evals=20):\n",
    "        self.max_evals = max_evals\n",
    "        self.n_evals = 0\n",
    "        self.f = f\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if self.n_evals >= self.max_evals:\n",
    "            raise Exception('Reached maximal number of evaliations')\n",
    "        else:\n",
    "            self.n_evals += 1\n",
    "\n",
    "        return self.f(*args, **kwargs)\n",
    "    \n",
    "x, _, history = numerical_gradient(\n",
    "    Function(eval_xgb_quality, 20),\n",
    "    x0=np.zeros(2),\n",
    "    \n",
    "    ### learning rate changed to 1e-1,\n",
    "    ### since new function has a much larger \"slope\".\n",
    "    learning_rate=1e-1, h=1e-1,\n",
    "    \n",
    "    n_evaluations=20,\n",
    "    progress=tqdm\n",
    ")\n",
    "\n",
    "test_quality = eval_xgb_quality(*x)\n",
    "\n",
    "assert test_quality < baseline_XGB, 'Looks like the optimization algorithm did not improved over baseline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 3: Simulated Annealing\n",
    "\n",
    "Implement simulated annealing:\n",
    "- use normally distribited noise with `std=learning_rate` for generating suggestions:\n",
    "  $$\\begin{eqnarray}\n",
    "  x'_{t + 1} &=& x_t + \\varepsilon;\\\\\n",
    "  \\varepsilon &\\sim& \\mathcal{N}(0, \\sigma^2 = \\mathrm{lr}^2)\n",
    "  \\end{eqnarray}$$\n",
    "\n",
    "- accept new point if:\n",
    "  $$x_{t + 1} = \\begin{cases}\n",
    "      x_{t + 1}, &\\text{ if } f(x'_{t + 1}) < f(x_{t});\\\\\n",
    "      x_{t + 1}, &\\text{ with } P = \\exp\\Big( \\frac{\\big(f(x_{t}) - f(x'_{t + 1})\\big)}{T} \\Big);\\\\\n",
    "      x_t  &\\text{ otherwise} \n",
    "  \\end{cases}$$\n",
    "\n",
    "- try different temperature schedules and learning_rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4eb0ed8f3939f92ff76d2709953b729",
     "grade": false,
     "grade_id": "SA",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
   ],
   "source": [
    "def simulated_annealing(\n",
    "    f, x0,\n",
    "    learning_rate=1e-1, T0=1e-1, T_final=1e-3,\n",
    "    n_evaluations=20, seed=100, progress=lambda x: x\n",
    "):\n",
    "\n",
    "    current_x = x0\n",
    "    current_f = f(*x0)\n",
    "    \n",
    "    history_x = [current_x]\n",
    "    history_f = [current_f]\n",
    "    \n",
    "    rng = np.random.RandomState(seed=seed)\n",
    "    \n",
    "    ### for numerical stability\n",
    "    ### instead of\n",
    "    ### exp((f1 - f0) / T) > unifrom(0, 1)\n",
    "    ### one can use \n",
    "    ### (f1 - f0) > np.log(unifrom(0, 1) + eps) * T\n",
    "    \n",
    "    for i in progress(range(n_evaluations - 1)):\n",
    "        T = T0 - (T0 - T_final) * i / (n_evaluations - 1)\n",
    "\n",
    "        suggestion = current_x + learning_rate * rng.normal(size=x0.shape)\n",
    "        f_suggestion = f(*suggestion)\n",
    "        \n",
    "        prob = (current_f - f_suggestion)\n",
    "        \n",
    "        if prob > np.log(rng.uniform(0, 1) + 1e-6) * T:\n",
    "            current_f = f_suggestion\n",
    "            current_x = suggestion\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        history_x.append(current_x)\n",
    "        history_f.append(current_f)\n",
    "    \n",
    "    best = np.argmin(history_f)\n",
    "    \n",
    "    return history_x[best], history_f[best], np.array(history_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "x, _, history = simulated_annealing(\n",
    "    log_resenbrock,\n",
    "    x0=np.array([0.5, 0.5]),\n",
    "    \n",
    "    learning_rate=5e-2, T0=1e-1, T_final=1e-3,\n",
    "    \n",
    "    n_evaluations=250,\n",
    "    seed=1111,\n",
    "    progress=tqdm,\n",
    ")\n",
    "\n",
    "assert np.all(np.abs(x - 1) < 2e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "plot(log_resenbrock, history, trajectory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "SA-XGB",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
   ],
   "source": [
    "class Function(object):\n",
    "    def __init__(self, f, max_evals=20):\n",
    "        self.max_evals = max_evals\n",
    "        self.n_evals = 0\n",
    "        self.f = f\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if self.n_evals >= self.max_evals:\n",
    "            raise Exception('Reached maximal number of evaliations')\n",
    "        else:\n",
    "            self.n_evals += 1\n",
    "\n",
    "        return self.f(*args, **kwargs)\n",
    "    \n",
    "x, _, history = simulated_annealing(\n",
    "    Function(eval_xgb_quality, 20),\n",
    "    x0=np.zeros(2),\n",
    "    \n",
    "    learning_rate=1e-1, T0=1e-1, T_final=1e-3,\n",
    "    \n",
    "    n_evaluations=20,\n",
    "    seed=1111,\n",
    "    progress=tqdm\n",
    ")\n",
    "\n",
    "test_quality = eval_xgb_quality(*x)\n",
    "\n",
    "assert test_quality < baseline_XGB, 'Looks like the optimization algorithm did not improved over baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}