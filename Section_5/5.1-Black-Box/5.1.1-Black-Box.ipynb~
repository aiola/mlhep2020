{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Black-Box optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import os\n",
    "\n",
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "CHUNK_SIZE = 1024 * 1024\n",
    "\n",
    "def download_and_save(url, path, warn=True, progress=None, cache_downloads=True):\n",
    "  import os\n",
    "\n",
    "  if os.path.exists(path):\n",
    "    raise IOError('Path %s already exists!' % path)\n",
    "\n",
    "  if cache_downloads:\n",
    "    import io\n",
    "    cache = io.BytesIO()\n",
    "  else:\n",
    "    cache = open(path, 'wb')\n",
    "\n",
    "  with cache:\n",
    "    response = requests.get(url, stream=True)\n",
    "    total = int(response.headers.get('content-length', None))\n",
    "\n",
    "    if warn:\n",
    "      import warnings\n",
    "      warnings.warn('Downloading %s: %s to %s' % (\n",
    "        total if total is not None else '[unknown size]',\n",
    "        url, path\n",
    "      ))\n",
    "\n",
    "    if progress is not None:\n",
    "      pbar = progress(total=total, desc=os.path.basename(path), unit='iB', unit_scale=True)\n",
    "      update = pbar.update\n",
    "    else:\n",
    "      update = lambda n: None\n",
    "\n",
    "    for chunk in response.iter_content(chunk_size=CHUNK_SIZE):\n",
    "      cache.write(chunk)\n",
    "      update(len(chunk))\n",
    "\n",
    "    if cache_downloads:\n",
    "      with open(path, 'wb') as f:\n",
    "        f.write(cache.getvalue())\n",
    "\n",
    "  return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def make_dataset(subsample=int(1e5)):\n",
    "    data_path = '../../data/SUSY.csv.gz'\n",
    "    data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz'\n",
    "\n",
    "    try:\n",
    "        download_and_save(url=data_url, path=data_path, progress=tqdm)\n",
    "    except IOError:\n",
    "        pass\n",
    "    \n",
    "    data = np.loadtxt(data_path, dtype='float32', delimiter=',')\n",
    "    \n",
    "    data_small = data[:subsample]\n",
    "    np.save('../../data/SUSY-small.npy', data_small)\n",
    "    \n",
    "    return data_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "try:\n",
    "    data = np.load('../../data/SUSY-small.npy')\n",
    "except FileNotFoundError:\n",
    "    data = make_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_all, y_all = data[:, 1:], data[:, 0]\n",
    "\n",
    "X_train, X_rest, y_train, y_rest = train_test_split(X_all, y_all, test_size=0.5)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_rest, y_rest, test_size=0.5)\n",
    "\n",
    "print('Train     :', X_train.shape, y_train.shape)\n",
    "print('Validation:', X_val.shape, y_val.shape)\n",
    "print('Test      :', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def plot(f, history=None, trajectory=False):\n",
    "    xs = np.linspace(-0.5, 2.5, num=250)\n",
    "    X, Y = np.meshgrid(xs, xs)\n",
    "\n",
    "    Z = np.stack([X.reshape(-1), Y.reshape(-1)], axis=1)\n",
    "    F = f(Z[:, 0], Z[:, 1]).reshape(xs.shape[0], xs.shape[0])\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.ylim([-0.5, 2.5])\n",
    "    plt.xlim([-0.5, 2.5])\n",
    "\n",
    "    plt.contour(\n",
    "        xs, xs, F,\n",
    "        levels=np.linspace(0, 4, num=20),\n",
    "        colors=[plt.cm.tab10(0)]\n",
    "    )\n",
    "    plt.scatter([1], [1], marker='*', s=500, color=plt.cm.tab10(3), zorder=5)\n",
    "    \n",
    "    if history is not None:\n",
    "        plt.scatter(history[:, 0], history[:, 1], color=plt.cm.tab10(1), s=100)\n",
    "        if trajectory:\n",
    "            plt.plot(history[:, 0], history[:, 1], color=plt.cm.tab10(1), lw=3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The first objective function\n",
    "\n",
    "The first objective function is one of the most popular benchmark functions --- the Resenbrock function.\n",
    "We slightly modify it by applying $\\chi \\to \\log (1 + \\chi)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def log_resenbrock(x1, x2):\n",
    "    return np.log1p(\n",
    "        (1 - x1) ** 2 + 1 * (x2 - x1 ** 2) ** 2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "plot(log_resenbrock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The second objective function\n",
    "\n",
    "One of use-cases for black-box optimization is hyper-parameter tuning.\n",
    "Here, we measure quality of XGBoost as a function of two continous parameters --- learning rate and regularization coefficient.\n",
    "The dataset is a small version of the [SUSY](https://archive.ics.uci.edu/ml/datasets/SUSY) dataset (loaded above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def eval_xgb_quality(log_learning_rate, log_reg_lambda):\n",
    "    learning_rate = np.exp(log_learning_rate)\n",
    "    reg_lambda = np.exp(log_reg_lambda)\n",
    "\n",
    "    clf = XGBClassifier(\n",
    "        learning_rate=learning_rate,\n",
    "        reg_lambda=reg_lambda,\n",
    "\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "\n",
    "        n_jobs=-1,\n",
    "        subsample=1e-1,\n",
    "        random_state=111\n",
    "    )\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict_proba(X_val)\n",
    "    \n",
    "    return 1 - accuracy_score(y_val, predictions[:, 1] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "%%time\n",
    "\n",
    "### baseline quality is for comparison\n",
    "baseline_XGB = eval_xgb_quality(0, 0)\n",
    "print('XGBoost baseline: %.3lf' % (baseline_XGB,) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 1: Random Search\n",
    "\n",
    "- implement sampling for Random Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d494622a524af495be28664e918ab339",
     "grade": false,
     "grade_id": "RS",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
   ],
   "source": [
    "def random_search(f, bounds, n_evaluations=20, progress=lambda x: x, seed=111222):\n",
    "    bounds = np.array(bounds)\n",
    "    \n",
    "    history_x = []\n",
    "    history_f = []\n",
    "    \n",
    "    rng = np.random.RandomState(seed=seed)\n",
    "    ### rng has the same methods as np.random,\n",
    "    ### e.g., rng.uniform - np.random.uniform\n",
    "    \n",
    "    for _ in progress(range(n_evaluations)):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        value = f(*x)\n",
    "        \n",
    "        history_x.append(x)\n",
    "        history_f.append(value)\n",
    "    \n",
    "    best = np.argmin(history_f)\n",
    "    history_x = np.array(history_x)\n",
    "    \n",
    "    return history_x[best], history_f[best], history_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "x, _, history = random_search(\n",
    "    log_resenbrock, \n",
    "    bounds=[(-0.5, 2.5), (-0.5, 2.5)],\n",
    "    progress=tqdm,\n",
    "    n_evaluations=1000\n",
    ")\n",
    "\n",
    "assert log_resenbrock(x[0], x[1]) < 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "plot(log_resenbrock, history, trajectory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "RS-XGB",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
   ],
   "source": [
    "x, error, history = random_search(eval_xgb_quality, bounds=[(-3, 3), (-3, 3)], progress=tqdm)\n",
    "\n",
    "test_quality = eval_xgb_quality(*x)\n",
    "\n",
    "assert test_quality < baseline_XGB, 'Looks like the optimization algorithm did not improved over baseline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 2: numerical gradient estimation\n",
    "\n",
    "- implement numerical gradient estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a992a3b0198bbf04a5ee1d619d246a8",
     "grade": false,
     "grade_id": "NG",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
   ],
   "source": [
    "def numerical_gradient(f, x0, learning_rate=1e-1, h=1e-2, n_evaluations=20, progress=lambda x: x):\n",
    "    n_iters = (n_evaluations - 1) // (1 + x0.shape[0])\n",
    "    current_x = x0\n",
    "    \n",
    "    history_x = [current_x]\n",
    "    \n",
    "    for _ in progress(range(n_iters)):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    return current_x, f(*current_x), np.array(history_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "x, _, history = numerical_gradient(\n",
    "    log_resenbrock,\n",
    "    x0=np.array([0.5, 0.5]),\n",
    "    progress=tqdm,\n",
    "    \n",
    "    n_evaluations=2500\n",
    ")\n",
    "\n",
    "assert np.all(np.abs(x - 1) < 1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "plot(log_resenbrock, history, trajectory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "NG-XGB",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
   ],
   "source": [
    "class Function(object):\n",
    "    def __init__(self, f, max_evals=20):\n",
    "        self.max_evals = max_evals\n",
    "        self.n_evals = 0\n",
    "        self.f = f\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if self.n_evals >= self.max_evals:\n",
    "            raise Exception('Reached maximal number of evaliations')\n",
    "        else:\n",
    "            self.n_evals += 1\n",
    "\n",
    "        return self.f(*args, **kwargs)\n",
    "    \n",
    "x, _, history = numerical_gradient(\n",
    "    Function(eval_xgb_quality, 20),\n",
    "    x0=np.zeros(2),\n",
    "\n",
    "    learning_rate=1e+1, h=1e-1,\n",
    "    \n",
    "    n_evaluations=20,\n",
    "    progress=tqdm\n",
    ")\n",
    "\n",
    "test_quality = eval_xgb_quality(*x)\n",
    "\n",
    "assert test_quality < baseline_XGB, 'Looks like the optimization algorithm did not improved over baseline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 3: Simulated Annealing\n",
    "\n",
    "Implement simulated annealing:\n",
    "- use normally distribited noise with `std=learning_rate` for generating suggestions:\n",
    "  $$\\begin{eqnarray}\n",
    "  x'_{t + 1} &=& x_t + \\varepsilon;\\\\\n",
    "  \\varepsilon &\\sim& \\mathcal{N}(0, \\sigma^2 = \\mathrm{lr}^2)\n",
    "  \\end{eqnarray}$$\n",
    "\n",
    "- accept new point if:\n",
    "  $$x_{t + 1} = \\begin{cases}\n",
    "      x_{t + 1}, &\\text{ if } f(x'_{t + 1}) < f(x_{t});\\\\\n",
    "      x_{t + 1}, &\\text{ with } P = \\exp\\Big( \\frac{\\big(f(x_{t}) - f(x'_{t + 1})\\big)}{T} \\Big);\\\\\n",
    "      x_t  &\\text{ otherwise} \n",
    "  \\end{cases}$$\n",
    "\n",
    "- try different temperature schedules and learning_rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81c02d7c6d8468c9bb605aa4cfd6ab0f",
     "grade": false,
     "grade_id": "SA",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
   ],
   "source": [
    "def simulated_annealing(\n",
    "    f, x0,\n",
    "    learning_rate=1e-1, T0=1e-1, T_final=1e-3,\n",
    "    n_evaluations=20, seed=100, progress=lambda x: x\n",
    "):\n",
    "\n",
    "    current_x = x0\n",
    "    current_f = f(*x0)\n",
    "    \n",
    "    history_x = [current_x]\n",
    "    history_f = [current_f]\n",
    "    \n",
    "    rng = np.random.RandomState(seed=seed)\n",
    "    \n",
    "    ### for numerical stability\n",
    "    ### instead of\n",
    "    ### exp((f1 - f0) / T) > unifrom(0, 1)\n",
    "    ### one can use \n",
    "    ### (f1 - f0) > np.log(unifrom(0, 1) + eps) * T\n",
    "    \n",
    "    for i in progress(range(n_evaluations - 1)):\n",
    "        T = T0 - (T0 - T_final) * i / (n_evaluations - 1)\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    best = np.argmin(history_f)\n",
    "    \n",
    "    return history_x[best], history_f[best], np.array(history_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "x, _, history = simulated_annealing(\n",
    "    log_resenbrock,\n",
    "    x0=np.array([0.5, 0.5]),\n",
    "    \n",
    "    learning_rate=5e-2, T0=1e-1, T_final=1e-3,\n",
    "    \n",
    "    n_evaluations=250,\n",
    "    seed=1111,\n",
    "    progress=tqdm,\n",
    ")\n",
    "\n",
    "assert np.all(np.abs(x - 1) < 2e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "plot(log_resenbrock, history, trajectory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "SA-XGB",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
   ],
   "source": [
    "class Function(object):\n",
    "    def __init__(self, f, max_evals=20):\n",
    "        self.max_evals = max_evals\n",
    "        self.n_evals = 0\n",
    "        self.f = f\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if self.n_evals >= self.max_evals:\n",
    "            raise Exception('Reached maximal number of evaliations')\n",
    "        else:\n",
    "            self.n_evals += 1\n",
    "\n",
    "        return self.f(*args, **kwargs)\n",
    "    \n",
    "x, _, history = simulated_annealing(\n",
    "    Function(eval_xgb_quality, 20),\n",
    "    x0=np.zeros(2),\n",
    "    \n",
    "    learning_rate=1e-1, T0=1e-1, T_final=1e-3,\n",
    "    \n",
    "    n_evaluations=20,\n",
    "    seed=1111,\n",
    "    progress=tqdm\n",
    ")\n",
    "\n",
    "test_quality = eval_xgb_quality(*x)\n",
    "\n",
    "assert test_quality < baseline_XGB, 'Looks like the optimization algorithm did not improved over baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}